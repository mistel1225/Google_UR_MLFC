adam_epsilon: 1.0e-08
early_stop_callback: false
eval_batch_size: 4
fp_16: false
gradient_accumulation_steps: 32
hparams:
  adam_epsilon: 1.0e-08
  early_stop_callback: false
  eval_batch_size: 4
  fp_16: false
  gradient_accumulation_steps: 32
  learning_rate: 0.0003
  limit_train_batches: false
  max_grad_norm: 1.0
  max_seq_length: 300
  model_name_or_path: bert-base-uncased
  n_gpu: 1
  num_labels: 8
  num_train_epochs: 8
  num_train_size: 15000
  opt_level: '01'
  output_dir: ./output
  sanity_val: true
  seed: 42
  tokenizer_name_or_path: bert-base-uncased
  train_batch_size: 16
  val_check_interval: 0.25
  warmup_steps: 0
  weight_decay: 0.0
learning_rate: 0.0003
limit_train_batches: false
max_grad_norm: 1.0
max_seq_length: 300
model_name_or_path: bert-base-uncased
n_gpu: 1
num_labels: 8
num_train_epochs: 8
num_train_size: 15000
opt_level: '01'
output_dir: ./output
sanity_val: true
seed: 42
tokenizer_name_or_path: bert-base-uncased
train_batch_size: 16
val_check_interval: 0.25
warmup_steps: 0
weight_decay: 0.0
